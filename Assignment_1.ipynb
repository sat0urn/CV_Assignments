{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sat0urn/CV_Assignments/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment \\#1**\n",
        "# ***Komekbayev Zeyin & Kusainov Aslan SE-2116***\n",
        "\n",
        "# **Loading \"*Agricultural Crops*\" Image Classification Dataset**"
      ],
      "metadata": {
        "id": "G153c6dTHt5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "glRa6TLPGNYW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os, cv2, random, math\n",
        "import pdb # For debugging\n",
        "\n",
        "# Loading Agricultural Crops Dataset\n",
        "\n",
        "def load_AGRI_CROPS(data_dir):\n",
        "    classes = os.listdir(data_dir)\n",
        "\n",
        "    images = []\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    for class_name in classes:\n",
        "      class_dir = os.path.join(data_dir, class_name)\n",
        "      for image_file in os.listdir(class_dir):\n",
        "        image_path = os.path.join(class_dir, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (64, 64))\n",
        "        image = image.flatten()\n",
        "        images.append(image)\n",
        "        labels.append(class_name)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Agricultural-crops'\n",
        "\n",
        "images, labels = load_AGRI_CROPS(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Softmax Regression Algorithm**"
      ],
      "metadata": {
        "id": "NP60b-gbo3A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Regression Algorithm Class\n",
        "class SoftmaxRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=100, regularization=None, lambda_reg=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.regularization = regularization\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape[0], X.shape[1]\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(num_features, num_classes)\n",
        "        self.bias = np.zeros((1, num_classes))\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            # Compute softmax scores\n",
        "            scores = X.dot(self.weights) + self.bias\n",
        "\n",
        "            # Compute softmax probabilities\n",
        "            max_scores = np.max(scores, axis=1, keepdims=True)\n",
        "            exp_scores = np.exp(scores - max_scores)\n",
        "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = -np.log(probs[range(num_samples), y] + 1e-7)\n",
        "            data_loss = np.sum(loss) / num_samples\n",
        "            reg_loss = 0.5 * self.lambda_reg * np.sum(self.weights ** 2)\n",
        "            total_loss = data_loss + reg_loss\n",
        "\n",
        "            # Compute gradients\n",
        "            delta = probs\n",
        "            delta[range(num_samples), y] -= 1\n",
        "            delta /= num_samples\n",
        "\n",
        "            dtheta = X.T.dot(delta)\n",
        "\n",
        "            db = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "            # Regularizations\n",
        "            if self.regularization == 'L2':\n",
        "                dtheta += self.lambda_reg * self.weights\n",
        "            elif self.regularization == 'L1':\n",
        "                dtheta += self.lambda_reg * np.sign(self.weights)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights -= self.learning_rate * dtheta\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.weights) + self.bias\n",
        "        return np.argmax(scores, axis=1)"
      ],
      "metadata": {
        "id": "WypDHNW8o8EN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM Algorithm**"
      ],
      "metadata": {
        "id": "mf5rpI1oo_Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Algorithm Class\n",
        "class SVM:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=100, regularization=None, lambda_reg=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.regularization = regularization\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape[0], X.shape[1]\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.rand(num_features, num_classes)\n",
        "        self.bias = np.zeros((1, num_classes))\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            # Calculate the raw scores for each class using\n",
        "            # dot product of training data and weights, then add bias\n",
        "            scores = X.dot(self.weights) + self.bias\n",
        "\n",
        "            # Calculate the scores of the correct class for each sample\n",
        "            correct_scores = scores[range(num_samples), y]\n",
        "\n",
        "            # Calculate margins, which represent the difference between\n",
        "            # the scores of the correct class and other classes\n",
        "            margins = np.maximum(0, scores - correct_scores[:, np.newaxis] + 1)\n",
        "            margins[range(num_samples), y] = 0\n",
        "            loss = np.sum(margins) / num_samples\n",
        "\n",
        "            # Compute gradients\n",
        "            binary = margins\n",
        "            binary[margins > 0] = 1\n",
        "            row_sum = np.sum(binary, axis=1)\n",
        "            binary[range(num_samples), y] = -row_sum\n",
        "            dtheta = X.T.dot(binary) / num_samples\n",
        "\n",
        "            if self.regularization == 'L2':\n",
        "                dtheta += self.lambda_reg * self.weights\n",
        "            elif self.regularization == 'L1':\n",
        "                dtheta += self.lambda_reg * np.sign(self.weights)\n",
        "\n",
        "            self.weights -= self.learning_rate * dtheta\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.weights) + self.bias\n",
        "        return np.argmax(scores, axis=1)"
      ],
      "metadata": {
        "id": "Urvh9wBXpBsR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Divide into different sets (train, validation, test)**"
      ],
      "metadata": {
        "id": "_BQyKR3ipFjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide data into train, validation, and test sets\n",
        "def train_val_test_split(X, y, val_size=0.1, test_size=0.1):\n",
        "    num_samples = X.shape[0]\n",
        "\n",
        "    # Initialize size of test and validation\n",
        "    val_size = int(num_samples * val_size)\n",
        "    test_size = int(num_samples * test_size)\n",
        "\n",
        "    indices = np.random.permutation(num_samples)\n",
        "\n",
        "    # Defining Indices\n",
        "    val_indices = indices[:val_size]\n",
        "    test_indices = indices[val_size:val_size+test_size]\n",
        "    train_indices = indices[val_size+test_size:]\n",
        "\n",
        "    # Split All Sets to Indices\n",
        "    X_val, y_val = X[val_indices], y[val_indices]\n",
        "    X_test, y_test = X[test_indices], y[test_indices]\n",
        "    X_train, y_train = X[train_indices], y[train_indices]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(images, labels, val_size=0.1, test_size=0.1)\n",
        "\n",
        "# Normalize the data\n",
        "class_to_label = {class_name: i for i, class_name in enumerate(np.unique(y_train))}\n",
        "label_to_class = {i: class_name for class_name, i in class_to_label.items()}\n",
        "\n",
        "# Convert class labels to integer labels\n",
        "y_train = np.array([class_to_label[class_name] for class_name in y_train])\n",
        "y_test = np.array([class_to_label[class_name] for class_name in y_test])\n",
        "y_val = np.array([class_to_label[class_name] for class_name in y_val])\n",
        "\n",
        "# Flatten the data\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "X_val = X_val.reshape(X_val.shape[0], -1)"
      ],
      "metadata": {
        "id": "koXwllHmpM1-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split Train into Train & Validation sets for Cross-Validation**"
      ],
      "metadata": {
        "id": "XwyUhcTypQv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Further split train into train and validation sets for cross-validation\n",
        "def k_fold_split(X, y, k=5):\n",
        "    fold_size = len(X) // k\n",
        "    for i in range(k):\n",
        "        start = i * fold_size\n",
        "        end = (i + 1) * fold_size\n",
        "        X_val = X[start:end]\n",
        "        y_val = y[start:end]\n",
        "        X_train = np.concatenate([X[:start], X[end:]])\n",
        "        y_train = np.concatenate([y[:start], y[end:]])\n",
        "        yield X_train, y_train, X_val, y_val"
      ],
      "metadata": {
        "id": "cBoJp3zzpQ_W"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculate Accuracy**"
      ],
      "metadata": {
        "id": "-8KHLo05pXu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate an accuracy\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)"
      ],
      "metadata": {
        "id": "HSDlYZ9FpaZR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models Initialization and Accuracy Evaluation**"
      ],
      "metadata": {
        "id": "x9L6aqelpfRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "softmax_model = SoftmaxRegression(learning_rate=0.01, n_iterations=100, regularization='L2', lambda_reg=0.001)\n",
        "svm_model = SVM(learning_rate=0.01, n_iterations=100, regularization='L2', lambda_reg=0.001)\n",
        "\n",
        "best_accuracy = -1\n",
        "best_algorithm = None\n",
        "\n",
        "softmax_accuracies = []\n",
        "svm_accuracies = []\n",
        "\n",
        "for X_train_fold, y_train_fold, X_val_fold, y_val_fold in k_fold_split(X_train, y_train, k=5):\n",
        "    # Train Softmax Regression\n",
        "    softmax_model.fit(X_train_fold, y_train_fold)\n",
        "    softmax_predictions = softmax_model.predict(X_val_fold)\n",
        "    softmax_accuracy = calculate_accuracy(y_val_fold, softmax_predictions)\n",
        "    softmax_accuracies.append(softmax_accuracy)\n",
        "\n",
        "    # Train SVM\n",
        "    svm_model.fit(X_train_fold, y_train_fold)\n",
        "    svm_predictions = svm_model.predict(X_val_fold)\n",
        "    svm_accuracy = calculate_accuracy(y_val_fold, svm_predictions)\n",
        "    svm_accuracies.append(svm_accuracy)\n",
        "\n",
        "average_softmax_accuracy = np.mean(softmax_accuracies)\n",
        "average_svm_accuracy = np.mean(svm_accuracies)\n",
        "\n",
        "if average_softmax_accuracy > best_accuracy:\n",
        "    best_accuracy = average_softmax_accuracy\n",
        "    best_algorithm = \"Softmax Regression Algorithm\"\n",
        "\n",
        "if average_svm_accuracy > best_accuracy:\n",
        "    best_accuracy = average_svm_accuracy\n",
        "    best_algorithm = \"SVM Algorithm\"\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "if best_algorithm == \"Softmax Regression Algorithm\":\n",
        "    softmax_model.fit(X_train, y_train)\n",
        "    test_predictions = softmax_model.predict(X_test)\n",
        "elif best_algorithm == \"SVM Algorithm\":\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "test_accuracy = calculate_accuracy(y_test, test_predictions)"
      ],
      "metadata": {
        "id": "CG0kQ-nGphrC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output / Result**"
      ],
      "metadata": {
        "id": "BAQHZH-EEav_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Algorithm Output Information\n",
        "print(f\"Algorithm with Best Accuracy: {best_algorithm}\")\n",
        "print(f\"Test Accuracy of {best_algorithm}: {test_accuracy}\")\n",
        "print(f\"Average Accuracy for Softmax Regression Algorithm in 5k-CV: {average_softmax_accuracy}\")\n",
        "print(f\"Average Accuracy for SVM Algorithm in 5k-CV: {average_svm_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNcR6YJ_EesC",
        "outputId": "92d27de8-af0e-4ea1-fe40-02354cf82a01"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Algorithm with Best Accuracy: SVM Algorithm\n",
            "Test Accuracy of SVM Algorithm: 0.24390243902439024\n",
            "Average Accuracy for Softmax Regression Algorithm in 5k-CV: 0.16240601503759397\n",
            "Average Accuracy for SVM Algorithm in 5k-CV: 0.18045112781954886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reason for SVM Algorithm for being More Accurant**\n",
        "\n",
        "So we can see that SVM is more suitable for large and complex high-dimensional dataset of images for classification than Softmax Regression. By end result it is showing their not being too far from each other in terms of accuracy score, but SVM takes more places in continouos checking."
      ],
      "metadata": {
        "id": "uI3K9PUjHbQy"
      }
    }
  ]
}